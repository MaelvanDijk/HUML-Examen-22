{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../tune')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 06:58:23.911 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humor': 4213, 'reuters': 4186, 'wiki': 4181, 'proverbs': 831}\n",
      "{'humor': 31.414510476474533, 'reuters': 31.21318320781448, 'wiki': 31.175900380284844, 'proverbs': 6.196405935426143}\n"
     ]
    }
   ],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "label_dict = {\n",
    "    \"humor\": 0,\n",
    "    \"reuters\": 0,\n",
    "    \"wiki\": 0,\n",
    "    \"proverbs\": 0\n",
    "}\n",
    "\n",
    "for row in traindataset.dataset:\n",
    "    label_dict[row[1]] += 1\n",
    "\n",
    "print(label_dict)\n",
    "\n",
    "for k in label_dict:\n",
    "    label_dict[k] =  label_dict[k] / len(traindataset.dataset) * 100\n",
    "\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Due to the disbalance in the classes (4th to be precise) the model will probably tend to fit on the first three classes as these are easier to guess. Mostly the minority class will cause problems if we need to predict these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"humor\":\n",
    "            return 0\n",
    "        elif label == \"reuters\":\n",
    "            return 1\n",
    "        elif label == \"wiki\":\n",
    "            return 2\n",
    "        elif label == \"proverbs\":\n",
    "            return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "x1, y1=preprocessor([(x, y)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 41]),\n",
       " tensor([1, 2, 1, 2, 3, 1, 0, 0, 2, 2, 1, 0, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 2,\n",
       "         0, 0, 1, 2, 1, 3, 1, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metric = metrics.F1Score()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class WeightedF1Score(metrics.Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"WeightedF1Score\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        yhat = yhat.argmax(dim=1)\n",
    "        score = f1_score(y, yhat, average=\"weighted\")\n",
    "        return torch.tensor(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.models.metrics.F1Score"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "from src.models import rnn\n",
    "\n",
    "config= {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 4,\n",
    "}\n",
    "\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=50,\n",
    "    model=model,\n",
    "    metrics=[metric],\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1.  13411 datapoints divided by batches of size 32 equals 419.09375 batches before the dataset could be fully seen.\n",
      "        Rounded up this would become 420\n",
      "    2.  Depending on the way we look at the question it can either be 20.955 or 21 epochs. \n",
      "        When we consider the train and test sets to be both run through the trainstep() function.\n",
      "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
      "        then we take the max between those two for a total of 16.76375 (17 when rounded) epochs.\n",
      "    3.  16.76375 or 17 rounded up.\n",
      "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
      "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
      "        we can go up to the humongous number below given basic combinatoric math \n",
      "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
      "        2553595427532697207896236658782847906869511112444955464584374200415869309443961050635633361494919486648172775020586949193836418057452631414395873061047299135306786342851710738555641861925710053699454898916976834843186820794236483017152787331685133409007759209619059384842491896515956129517517163230433337293580498877173735399759757414225084486158140323211053136995791580146060372088374698374755502009016909723850264804283137696375156069094542087730127734747934619245781360199990713961550070797279732888313735307808089033960068946140351729178052128574224151260561895466401459994851285413481732269348863667092355973771930336201103362858691765942903007498770215035417647192799812545990968607367963342646698291656634277284462197604715130710122390060848519718582729317581691333698440433739196687218300909891438014576087197704989419270656984509182985166240686617288733354930896980115700363118288048476431001473029834943976567552146584306952984978744678337273323711187900518071084636671616759626099013124161586231615520221857825905458040683266900653634796593139003481696910181381209934792133549800699813325097453348011915974203791830134162508241371571861313457786276213660433504634786686332541274004003608791390234512906185182479198673837206892256695181088027464244505060519652996131772728880126665665263717526304778979572590569861214649028877419335079290478006763052946286446748781381617188339600271081055403180471724461809622440649158445199072739216936530364086002636104286125758375893649767174729801407936700394156076664338415783722758433370184782025676136800468007128895341604771838325019061855276537180768726575973288754696198654192540791451497686218955230242661795009046648031584150437966475345883096810605264000414511939467950087420138312960911898409447945899642115279857445748286910153943444151744509714113476610776453637793879944601506953734026167299304096848706293275717511175655277953811567292212634750221404541865692829205098157446333804482276548606594031255806228391152552579147493555624767133109274949887199037727073917427285384044155254174434486109737854503427473815914623018386624565997480538487570571804284639462718785769543304146737806671745571086640649496021606936661147899860117808465264351881748123359681192664851591394817416472091093758352442506201419455543051236854175914565614993688543625546931339538358879869134125011454955196880854947717508634423022409914158134535012378354539328643167551799538311731444957613416128761096533116649358184682984082262392334544274681596591882148850798122259960660196747629356329123131973309290281165338808071412330039995720551485164229573899579176297258172060811621459760701386928190117143774321648430772931004954984841002375678259554874694525205917381076885515353917655224155446233745650390234206415145998464522857174592928885427435581829278780824799778437724972195205361350302708666448977830897280840083529213362790397174110042172977446706799195498559864776309446101139848590957399534935291397332282849639336964395744356713820166574680291085640954165938556374407377343902405191387580424737866590661743412347873070797724533856153488312814796800000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
      "        we can see only marginally smaal improvements after epoch 25.\n",
      "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
      "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
      "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\n",
    "    f'''\n",
    "    1.  {len(traindataset)} datapoints divided by batches of size 32 equals {len(traindataset) / 32} batches before the dataset could be fully seen.\n",
    "        Rounded up this would become 420\n",
    "    2.  Depending on the way we look at the question it can either be {(len(traindataset)  + len(testdataset)) / (25 * 32)} or 21 epochs. \n",
    "        When we consider the train and test sets to be both run through the trainstep() function.\n",
    "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
    "        then we take the max between those two for a total of {max(len(testdataset) / (25*32),\n",
    "     len(traindataset) / (25*32))} (17 when rounded) epochs.\n",
    "    3.  {len(traindataset) / (25*32)} or 17 rounded up.\n",
    "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
    "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
    "        we can go up to the humongous number below given basic combinatoric math \n",
    "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
    "        {math.factorial(len(traindataset)) // math.factorial((len(traindataset) - (25*32)))}\n",
    "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
    "        we can see only marginally smaal improvements after epoch 25.\n",
    "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
    "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
    "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir \"/home/mladmin/code/eind_opdracht/examen-22/tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 15.0, 'Predicted'), Text(33.0, 0.5, 'Target')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The confusionmatrix shows the accuracy (as per the documentation of scikitlearn). This confusion matrix shows the a multiclass classification instead of the standard binary variant. Within each of the rows in this matrix the total should be 1 (or 100%). \n",
    "2. I would prefere the F1 metric or even better the F1 weighted metric to have a better feeling how the model performs overal. But, because we know right now that the dataset is imbalanced the accuracy is not a bad score to check our performance and get an idea of the model and if it is performing better than a dumb model.\n",
    "3. because of the data imbalance we know that acc should be higher than ~30 percent for the majority classes and higher than ~5 percent for the minority class. This in turn tells us that the current model is doing a better job than pure gambling\n",
    "4. We could try under or over sampling (I would say under sampling because I do not have the knoweledge regarding over sampling in NLP). Other ways to combat this is maybe using the weighted F1 metric (giving the minority class as much importance as the majority classes) or a weighted loss function. But this wholy depends on the goal of the project. If errors in the minority class are of no problem then tackling this problem give rise to different solutions than when the minority class is the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.layers import combinators as cb\n",
    "from trax.shapes import signature\n",
    "from trax.supervised import training\n",
    "from trax.supervised.lr_schedules import warmup_and_rsqrt_decay\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 50), (32,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=50, vocab=v, clean=tokenizer.clean)\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset,\n",
    "    batchsize=32,\n",
    "    preprocessor=preprocessor\n",
    ").stream()\n",
    "\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset,\n",
    "    batchsize=32,\n",
    "    preprocessor=preprocessor\n",
    ").stream()\n",
    "\n",
    "def Cast():\n",
    "    '''\n",
    "    NOTE MAEL:\n",
    "    This function casts an input tensor to an numpy array.\n",
    "    During this process the numpy array will be padded to a fixed lenght\n",
    "    Because the stylepreprocessor uses the torch function pad_sequence()\n",
    "    max length within the current batch, I had to work around this.\n",
    "    changing the tokenizer could be an option but seeing other notebooks might use this\n",
    "    I opted to change this function instead. \n",
    "\n",
    "    The cause to implement this was a trax error where every batch yielded different sized tensors as input X\n",
    "    '''\n",
    "    def f(generator, max_len=50):\n",
    "        for x, y in generator:\n",
    "            new_x= []\n",
    "            _x = x.numpy()\n",
    "            for i, emb in enumerate(_x):\n",
    "                np_array_to_pad = emb\n",
    "                new_x.append(np.pad(np_array_to_pad, (0, max_len - len(np_array_to_pad))))\n",
    "            yield np.array(new_x), y.numpy()\n",
    "        return new_x\n",
    "    return lambda g: f(g)\n",
    "\n",
    "data_pipeline = trax.data.Serial(Cast())\n",
    "trainpipe = data_pipeline(trainstreamer)\n",
    "testpipe = data_pipeline(teststreamer)\n",
    "data= next(trainpipe)\n",
    "data[0].shape, data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 model 1 basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(4)\n",
    ")\n",
    "\n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n",
    "base_model._clear_init_cache()\n",
    "\n",
    "# print(X.shape)\n",
    "# base_model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 model 2 with larger first dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50)\n"
     ]
    }
   ],
   "source": [
    "base_model2 = cb.Serial(\n",
    "    tl.Dense(256),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(4)\n",
    ")\n",
    "\n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n",
    "\n",
    "base_model.init_weights_and_state(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-Z0MsbPIA-py3.9/lib/python3.9/site-packages/jax/_src/lib/xla_bridge.py:514: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trax.supervised import training\n",
    "from pathlib import Path\n",
    "\n",
    "# del(train_task, eval_task, loop)\n",
    "\n",
    "OUTPUT_PATH = Path(\"../tune_trax/base_model3\")\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=trainpipe,\n",
    "    loss_layer=tl.CategoryCrossEntropy(),\n",
    "    optimizer=trax.optimizers.Adam(),\n",
    "    lr_schedule=lr\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=testpipe,\n",
    "    metrics=[tl.CategoryAccuracy(), tl.WeightedFScore(), tl.MacroAveragedFScore(), tl.CategoryCrossEntropy()],\n",
    "    n_eval_batches=10\n",
    ")\n",
    "\n",
    "loop = training.Loop(\n",
    "    base_model3,\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=OUTPUT_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "LayerError",
     "evalue": "Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/trax/supervised/training.py, line 1033\n  layer input shapes: (ShapeDtype{shape:(32, 50), dtype:int32}, ShapeDtype{shape:(32,), dtype:int32})\n\n  File [...]/trax/layers/combinators.py, line 90, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer f1_loss (in pure_fn):\n  layer created in file [...]/tmp/ipykernel_3070/2119759075.py, line 17\n  layer input shapes: (ShapeDtype{shape:(32, 4), dtype:float32}, ShapeDtype{shape:(32,), dtype:int32})\n\n  File [...]/trax/layers/base.py, line 743, in forward\n    raw_output = self._forward_fn(inputs)\n\n  File [...]/trax/layers/base.py, line 784, in _forward\n    return f(*xs)\n\n  File [...]/tmp/ipykernel_3070/2119759075.py, line 5, in f\n    tp = jnp.sum(y_true * y_pred, axis= 0)\n\n  File [...]/site-packages/jax/core.py, line 571, in __mul__\n    def __mul__(self, other): return self.aval._mul(self, other)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4557, in deferring_binary_op\n    return binary_op(self, other)\n\n  File [...]/jax/_src/traceback_util.py, line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n\n  File [...]/jax/_src/api.py, line 473, in cache_miss\n    out_flat = xla.xla_call(\n\n  File [...]/site-packages/jax/core.py, line 1765, in bind\n    return call_bind(self, fun, *args, **params)\n\n  File [...]/site-packages/jax/core.py, line 1781, in call_bind\n    outs = top_trace.process_call(primitive, fun_, tracers, params)\n\n  File [...]/jax/interpreters/partial_eval.py, line 1583, in process_call\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n\n  File [...]/jax/interpreters/partial_eval.py, line 1865, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n\n  File [...]/site-packages/jax/linear_util.py, line 168, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n\n  File [...]/_src/numpy/ufuncs.py, line 86, in fn\n    x1, x2 = _promote_args(numpy_fn.__name__, x1, x2)\n\n  File [...]/_src/numpy/util.py, line 318, in _promote_args\n    return _promote_shapes(fun_name, *_promote_dtypes(*args))\n\n  File [...]/_src/numpy/util.py, line 227, in _promote_shapes\n    result_rank = len(lax.broadcast_shapes(*shapes))\n\n  File [...]/_src/lax/lax.py, line 122, in broadcast_shapes\n    return _broadcast_shapes_uncached(*shapes)\n\n  File [...]/_src/lax/lax.py, line 142, in _broadcast_shapes_uncached\n    raise ValueError(\"Incompatible shapes for broadcasting: {}\"\n\nValueError: Incompatible shapes for broadcasting: ((32, 4), (1, 32))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb Cell 52'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000050vscode-remote?line=0'>1</a>\u001b[0m loop\u001b[39m.\u001b[39;49mrun(\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-Z0MsbPIA-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:435\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m task_changed:\n\u001b[1;32m    433\u001b[0m   loss_acc, step_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m--> 435\u001b[0m loss, optimizer_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_step(task_index, task_changed)\n\u001b[1;32m    437\u001b[0m \u001b[39m# optimizer_metrics and loss are replicated on self.n_devices, a few\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m# metrics are replicated (ex: gradients_l2, weights_l2) - i.e. they are\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m# the same across devices, whereas some (ex: loss) aren't because they\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39m# implies the loss here is averaged from this hosts' devices and not\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m# across all hosts.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m optimizer_metrics, loss \u001b[39m=\u001b[39m fastmath\u001b[39m.\u001b[39mnested_map(\n\u001b[1;32m    447\u001b[0m     functools\u001b[39m.\u001b[39mpartial(tl\u001b[39m.\u001b[39mmean_or_pmean, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_devices),\n\u001b[1;32m    448\u001b[0m     (optimizer_metrics, loss))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-Z0MsbPIA-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:632\u001b[0m, in \u001b[0;36mLoop._run_one_step\u001b[0;34m(self, task_index, task_changed)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m task_changed:\n\u001b[1;32m    629\u001b[0m   \u001b[39m# Re-replicate weights and state to synchronize them between tasks.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_weights_and_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mweights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mstate)\n\u001b[0;32m--> 632\u001b[0m (loss, stats) \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mone_step(\n\u001b[1;32m    633\u001b[0m     batch, rng, step\u001b[39m=\u001b[39;49mstep, learning_rate\u001b[39m=\u001b[39;49mlearning_rate\n\u001b[1;32m    634\u001b[0m )\n\u001b[1;32m    636\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callbacks:\n\u001b[1;32m    637\u001b[0m   \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39mcall_at(step):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-Z0MsbPIA-py3.9/lib/python3.9/site-packages/trax/optimizers/trainer.py:147\u001b[0m, in \u001b[0;36mTrainer.one_step\u001b[0;34m(self, batch, rng, step, learning_rate)\u001b[0m\n\u001b[1;32m    144\u001b[0m   logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mstate[\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, state)\n\u001b[1;32m    146\u001b[0m \u001b[39m# NOTE: stats is a replicated dictionary of key to jnp arrays.\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m (new_weights, new_slots), new_state, stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accelerated_update_fn(\n\u001b[1;32m    148\u001b[0m     (weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slots), step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opt_params, batch, state, rng)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m logging\u001b[39m.\u001b[39mvlog_is_on(\u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m ((step \u001b[39m&\u001b[39m step \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    151\u001b[0m   logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mupdated weights[\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, new_weights)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-Z0MsbPIA-py3.9/lib/python3.9/site-packages/trax/optimizers/trainer.py:217\u001b[0m, in \u001b[0;36m_accelerate_update_fn.<locals>.single_device_update_fn\u001b[0;34m(weights_and_slots, step, opt_params, batch, state, rng)\u001b[0m\n\u001b[1;32m    215\u001b[0m step \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray(step, dtype\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mint32)  \u001b[39m# Needed in TFNP backend.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m weights, slots \u001b[39m=\u001b[39m weights_and_slots\n\u001b[0;32m--> 217\u001b[0m (loss, state), gradients \u001b[39m=\u001b[39m forward_and_backward_fn(\n\u001b[1;32m    218\u001b[0m     batch, weights, state, rng)\n\u001b[1;32m    219\u001b[0m weights, slots, stats \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mtree_update(\n\u001b[1;32m    220\u001b[0m     step, gradients, weights, slots, opt_params, store_slots\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    221\u001b[0m stats[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-Z0MsbPIA-py3.9/lib/python3.9/site-packages/trax/layers/base.py:605\u001b[0m, in \u001b[0;36mLayer.pure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m   \u001b[39m# Skipping 3 lines as it's always the uninteresting internal call.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m   name, trace \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, _short_traceback(skip\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m   \u001b[39mraise\u001b[39;00m LayerError(name, \u001b[39m'\u001b[39m\u001b[39mpure_fn\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    606\u001b[0m                    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_caller, signature(x), trace) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/trax/supervised/training.py, line 1033\n  layer input shapes: (ShapeDtype{shape:(32, 50), dtype:int32}, ShapeDtype{shape:(32,), dtype:int32})\n\n  File [...]/trax/layers/combinators.py, line 90, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer f1_loss (in pure_fn):\n  layer created in file [...]/tmp/ipykernel_3070/2119759075.py, line 17\n  layer input shapes: (ShapeDtype{shape:(32, 4), dtype:float32}, ShapeDtype{shape:(32,), dtype:int32})\n\n  File [...]/trax/layers/base.py, line 743, in forward\n    raw_output = self._forward_fn(inputs)\n\n  File [...]/trax/layers/base.py, line 784, in _forward\n    return f(*xs)\n\n  File [...]/tmp/ipykernel_3070/2119759075.py, line 5, in f\n    tp = jnp.sum(y_true * y_pred, axis= 0)\n\n  File [...]/site-packages/jax/core.py, line 571, in __mul__\n    def __mul__(self, other): return self.aval._mul(self, other)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4557, in deferring_binary_op\n    return binary_op(self, other)\n\n  File [...]/jax/_src/traceback_util.py, line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n\n  File [...]/jax/_src/api.py, line 473, in cache_miss\n    out_flat = xla.xla_call(\n\n  File [...]/site-packages/jax/core.py, line 1765, in bind\n    return call_bind(self, fun, *args, **params)\n\n  File [...]/site-packages/jax/core.py, line 1781, in call_bind\n    outs = top_trace.process_call(primitive, fun_, tracers, params)\n\n  File [...]/jax/interpreters/partial_eval.py, line 1583, in process_call\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n\n  File [...]/jax/interpreters/partial_eval.py, line 1865, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n\n  File [...]/site-packages/jax/linear_util.py, line 168, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n\n  File [...]/_src/numpy/ufuncs.py, line 86, in fn\n    x1, x2 = _promote_args(numpy_fn.__name__, x1, x2)\n\n  File [...]/_src/numpy/util.py, line 318, in _promote_args\n    return _promote_shapes(fun_name, *_promote_dtypes(*args))\n\n  File [...]/_src/numpy/util.py, line 227, in _promote_shapes\n    result_rank = len(lax.broadcast_shapes(*shapes))\n\n  File [...]/_src/lax/lax.py, line 122, in broadcast_shapes\n    return _broadcast_shapes_uncached(*shapes)\n\n  File [...]/_src/lax/lax.py, line 142, in _broadcast_shapes_uncached\n    raise ValueError(\"Incompatible shapes for broadcasting: {}\"\n\nValueError: Incompatible shapes for broadcasting: ((32, 4), (1, 32))"
     ]
    }
   ],
   "source": [
    "loop.run(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-25 07:32:09.133285: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-25 07:32:11.450811: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-25 07:32:11.450846: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.9.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir \"/home/mladmin/code/eind_opdracht/examen-22/tune_trax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Discussion\n",
    "\n",
    "Due to trax beign the most unfamilair to me I tried implementing 3 trax models. Due to the way the StyleSettings class was set up it was not possible to train multiple models. All config and setting files would be saved to the ../tune folder. This made it impossible to run the train loop on two seperate models. To remedy this I decided to opt for changing the log_dir to a variable that could more closly be controlled. the ../tune/model_1 and ../tune/model_2 folders proofs this is working as intendended.\n",
    "\n",
    "The first model was only for testing a basic architecture using Trax. It only has 3 layers: \n",
    "1. a Dense(128) layer\n",
    "2. a RELU activation layer\n",
    "3. a Dense(4) layer\n",
    "\n",
    "The second model only enlarged the first Dense layer from 128 to 256. This allowed me to test the checkpoint saving and loading used by Trax's loop.run(). Looking at the eval tensorboard files for both models We can at least see that the model is learning a bit more due to the increased size of the first layer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-Z0MsbPIA-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abd276725262becaa5c3e256f8c38384c11f1a26b31876c2a00eb7476ce26550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
