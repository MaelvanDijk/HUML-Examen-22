{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 17:43:57.983 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humor': 4213, 'reuters': 4186, 'wiki': 4181, 'proverbs': 831}\n",
      "{'humor': 31.414510476474533, 'reuters': 31.21318320781448, 'wiki': 31.175900380284844, 'proverbs': 6.196405935426143}\n"
     ]
    }
   ],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "label_dict = {\n",
    "    \"humor\": 0,\n",
    "    \"reuters\": 0,\n",
    "    \"wiki\": 0,\n",
    "    \"proverbs\": 0\n",
    "}\n",
    "\n",
    "for row in traindataset.dataset:\n",
    "    label_dict[row[1]] += 1\n",
    "\n",
    "print(label_dict)\n",
    "\n",
    "for k in label_dict:\n",
    "    label_dict[k] =  label_dict[k] / len(traindataset.dataset) * 100\n",
    "\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Due to the disbalance in the classes (4th to be precise) the model will probably tend to fit on the first three classes as these are easier to guess. Mostly the minority class will cause problems if we need to predict these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"humor\":\n",
    "            return 0\n",
    "        elif label == \"reuters\":\n",
    "            return 1\n",
    "        elif label == \"wiki\":\n",
    "            return 2\n",
    "        elif label == \"proverbs\":\n",
    "            return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4929,  854,   32,   15,  499,   21, 8496,  890]], dtype=torch.int32),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "preprocessor([(x, y)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 29]),\n",
       " tensor([2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 0, 1, 2, 1, 1, 0, 2, 1, 0, 0, 0, 2, 0, 1,\n",
       "         0, 0, 1, 0, 0, 2, 1, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metric = metrics.F1Score()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class WeightedF1Score(metrics.Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"WeightedF1Score\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        yhat = yhat.argmax(dim=1)\n",
    "        score = f1_score(y, yhat, average=\"weighted\")\n",
    "        return torch.tensor(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.models.metrics.F1Score"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "from src.models import rnn\n",
    "\n",
    "config= {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 4,\n",
    "}\n",
    "\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=50,\n",
    "    model=model,\n",
    "    metrics=[metric],\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1.  13411 datapoints divided by batches of size 32 equals 419.09375 batches before the dataset could be fully seen.\n",
      "        Rounded up this would become 420\n",
      "    2.  Depending on the way we look at the question it can either be 20.955 or 21 epochs. \n",
      "        When we consider the train and test sets to be both run through the trainstep() function.\n",
      "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
      "        then we take the max between those two for a total of 16.76375 (17 when rounded) epochs.\n",
      "    3.  16.76375 or 17 rounded up.\n",
      "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
      "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
      "        we can go up to the humongous number below given basic combinatoric math \n",
      "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
      "        2553595427532697207896236658782847906869511112444955464584374200415869309443961050635633361494919486648172775020586949193836418057452631414395873061047299135306786342851710738555641861925710053699454898916976834843186820794236483017152787331685133409007759209619059384842491896515956129517517163230433337293580498877173735399759757414225084486158140323211053136995791580146060372088374698374755502009016909723850264804283137696375156069094542087730127734747934619245781360199990713961550070797279732888313735307808089033960068946140351729178052128574224151260561895466401459994851285413481732269348863667092355973771930336201103362858691765942903007498770215035417647192799812545990968607367963342646698291656634277284462197604715130710122390060848519718582729317581691333698440433739196687218300909891438014576087197704989419270656984509182985166240686617288733354930896980115700363118288048476431001473029834943976567552146584306952984978744678337273323711187900518071084636671616759626099013124161586231615520221857825905458040683266900653634796593139003481696910181381209934792133549800699813325097453348011915974203791830134162508241371571861313457786276213660433504634786686332541274004003608791390234512906185182479198673837206892256695181088027464244505060519652996131772728880126665665263717526304778979572590569861214649028877419335079290478006763052946286446748781381617188339600271081055403180471724461809622440649158445199072739216936530364086002636104286125758375893649767174729801407936700394156076664338415783722758433370184782025676136800468007128895341604771838325019061855276537180768726575973288754696198654192540791451497686218955230242661795009046648031584150437966475345883096810605264000414511939467950087420138312960911898409447945899642115279857445748286910153943444151744509714113476610776453637793879944601506953734026167299304096848706293275717511175655277953811567292212634750221404541865692829205098157446333804482276548606594031255806228391152552579147493555624767133109274949887199037727073917427285384044155254174434486109737854503427473815914623018386624565997480538487570571804284639462718785769543304146737806671745571086640649496021606936661147899860117808465264351881748123359681192664851591394817416472091093758352442506201419455543051236854175914565614993688543625546931339538358879869134125011454955196880854947717508634423022409914158134535012378354539328643167551799538311731444957613416128761096533116649358184682984082262392334544274681596591882148850798122259960660196747629356329123131973309290281165338808071412330039995720551485164229573899579176297258172060811621459760701386928190117143774321648430772931004954984841002375678259554874694525205917381076885515353917655224155446233745650390234206415145998464522857174592928885427435581829278780824799778437724972195205361350302708666448977830897280840083529213362790397174110042172977446706799195498559864776309446101139848590957399534935291397332282849639336964395744356713820166574680291085640954165938556374407377343902405191387580424737866590661743412347873070797724533856153488312814796800000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
      "        we can see only marginally smaal improvements after epoch 25.\n",
      "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
      "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
      "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\n",
    "    f'''\n",
    "    1.  {len(traindataset)} datapoints divided by batches of size 32 equals {len(traindataset) / 32} batches before the dataset could be fully seen.\n",
    "        Rounded up this would become 420\n",
    "    2.  Depending on the way we look at the question it can either be {(len(traindataset)  + len(testdataset)) / (25 * 32)} or 21 epochs. \n",
    "        When we consider the train and test sets to be both run through the trainstep() function.\n",
    "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
    "        then we take the max between those two for a total of {max(len(testdataset) / (25*32),\n",
    "     len(traindataset) / (25*32))} (17 when rounded) epochs.\n",
    "    3.  {len(traindataset) / (25*32)} or 17 rounded up.\n",
    "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
    "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
    "        we can go up to the humongous number below given basic combinatoric math \n",
    "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
    "        {math.factorial(len(traindataset)) // math.factorial((len(traindataset) - (25*32)))}\n",
    "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
    "        we can see only marginally smaal improvements after epoch 25.\n",
    "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
    "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
    "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-23 17:44:06.347665: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-23 17:44:06.347699: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-06-23 17:44:08.697027: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-23 17:44:08.697121: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-23 17:44:08.697199: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ML-RefVm-812132): /proc/driver/nvidia/version does not exist\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.9.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir \"/home/mladmin/code/eind_opdracht/examen-22/tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 15.0, 'Predicted'), Text(33.0, 0.5, 'Target')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The confusionmatrix shows the accuracy (as per the documentation of scikitlearn). This confusion matrix shows the a multiclass classification instead of the standard binary variant. Within each of the rows in this matrix the total should be 1 (or 100%). \n",
    "2. I would prefere the F1 metric or even better the F1 weighted metric to have a better feeling how the model performs overal. But, because we know right now that the dataset is imbalanced the accuracy is not a bad score to check our performance and get an idea of the model and if it is performing better than a dumb model.\n",
    "3. because of the data imbalance we know that acc should be higher than ~30 percent for the majority classes and higher than ~5 percent for the minority class. This in turn tells us that the current model is doing a better job than pure gambling\n",
    "4. We could try under or over sampling (I would say under sampling because I do not have the knoweledge regarding over sampling in NLP). Other ways to combat this is maybe using the weighted F1 metric (giving the minority class as much importance as the majority classes) or a weighted loss function. But this wholy depends on the goal of the project. If errors in the minority class are of no problem then tackling this problem give rise to different solutions than when the minority class is the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.layers import combinators as cb\n",
    "from trax.shapes import signature\n",
    "from trax.supervised import training\n",
    "from trax.supervised.lr_schedules import warmup_and_rsqrt_decay\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=6, vocab=v, clean=tokenizer.clean)\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "\n",
    "\n",
    "def Cast():\n",
    "    def f(generator):\n",
    "        for x, y in generator:\n",
    "            yield x.numpy(), y.numpy()\n",
    "\n",
    "    return lambda g: f(g)\n",
    "\n",
    "data_pipeline = trax.data.Serial(Cast())\n",
    "trainpipe = data_pipeline(trainstreamer)\n",
    "testpipe = data_pipeline(teststreamer)\n",
    "X, y = next(trainpipe)\n",
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 6)\n"
     ]
    }
   ],
   "source": [
    "base_model = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(4)\n",
    ")\n",
    "\n",
    "loss = tl.CategoryCrossEntropy()\n",
    "score = tl.WeightedFScore()\n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n",
    "\n",
    "\n",
    "log_dir = settings.log_dir\n",
    "\n",
    "base_model.init_weights_and_state(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = training.TrainTask(\n",
    "    labeled_data=trainpipe,\n",
    "    loss_layer=loss,\n",
    "    optimizer=trax.optimizers.Adam(),\n",
    "    lr_schedule=lr\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=testpipe, metrics=[score, loss], n_eval_batches=25\n",
    ")\n",
    "\n",
    "loop = training.Loop(\n",
    "    base_model,\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=log_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    400: Ran 100 train steps in 0.19 secs\n",
      "Step    400: train CategoryCrossEntropy |  52.77430725\n",
      "Step    400: eval        WeightedFScore |  0.34902982\n",
      "Step    400: eval  CategoryCrossEntropy |  39.07955940\n",
      "\n",
      "Step    500: Ran 100 train steps in 0.16 secs\n",
      "Step    500: train CategoryCrossEntropy |  34.69036102\n",
      "Step    500: eval        WeightedFScore |  0.35342835\n",
      "Step    500: eval  CategoryCrossEntropy |  30.59502151\n",
      "\n",
      "Step    600: Ran 100 train steps in 0.18 secs\n",
      "Step    600: train CategoryCrossEntropy |  30.81590843\n",
      "Step    600: eval        WeightedFScore |  0.38952034\n",
      "Step    600: eval  CategoryCrossEntropy |  16.60132130\n",
      "\n",
      "Step    700: Ran 100 train steps in 0.16 secs\n",
      "Step    700: train CategoryCrossEntropy |  23.15695000\n",
      "Step    700: eval        WeightedFScore |  0.36689978\n",
      "Step    700: eval  CategoryCrossEntropy |  13.98621899\n",
      "\n",
      "Step    800: Ran 100 train steps in 0.25 secs\n",
      "Step    800: train CategoryCrossEntropy |  19.12630844\n",
      "Step    800: eval        WeightedFScore |  0.40928786\n",
      "Step    800: eval  CategoryCrossEntropy |  10.81515396\n"
     ]
    }
   ],
   "source": [
    "loop.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-23 17:50:13.613755: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-23 17:50:16.044496: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-23 17:50:16.044540: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.9.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir \"/home/mladmin/code/eind_opdracht/examen-22/tune\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-Z0MsbPIA-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abd276725262becaa5c3e256f8c38384c11f1a26b31876c2a00eb7476ce26550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
