{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../tune')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 14:14:53.944 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humor': 4213, 'reuters': 4186, 'wiki': 4181, 'proverbs': 831}\n",
      "{'humor': 31.414510476474533, 'reuters': 31.21318320781448, 'wiki': 31.175900380284844, 'proverbs': 6.196405935426143}\n"
     ]
    }
   ],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "label_dict = {\n",
    "    \"humor\": 0,\n",
    "    \"reuters\": 0,\n",
    "    \"wiki\": 0,\n",
    "    \"proverbs\": 0\n",
    "}\n",
    "\n",
    "for row in traindataset.dataset:\n",
    "    label_dict[row[1]] += 1\n",
    "\n",
    "print(label_dict)\n",
    "\n",
    "for k in label_dict:\n",
    "    label_dict[k] =  label_dict[k] / len(traindataset.dataset) * 100\n",
    "\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Due to the disbalance in the classes (4th to be precise) the model will probably tend to fit on the first three classes as these are easier to guess. Mostly the minority class will cause problems if we need to predict these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"humor\":\n",
    "            return 0\n",
    "        elif label == \"reuters\":\n",
    "            return 1\n",
    "        elif label == \"wiki\":\n",
    "            return 2\n",
    "        elif label == \"proverbs\":\n",
    "            return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "x1, y1=preprocessor([(x, y)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 31]),\n",
       " tensor([2, 0, 1, 3, 1, 2, 0, 2, 2, 2, 3, 0, 1, 1, 2, 0, 0, 2, 2, 1, 1, 1, 1, 0,\n",
       "         2, 2, 1, 0, 1, 1, 1, 2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metric = metrics.F1Score()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class WeightedF1Score(metrics.Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"WeightedF1Score\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        yhat = yhat.argmax(dim=1)\n",
    "        score = f1_score(y, yhat, average=\"weighted\")\n",
    "        return torch.tensor(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.models.metrics.F1Score"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "from src.models import rnn\n",
    "\n",
    "config= {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 4,\n",
    "}\n",
    "\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=50,\n",
    "    model=model,\n",
    "    metrics=[metric],\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1.  13411 datapoints divided by batches of size 32 equals 419.09375 batches before the dataset could be fully seen.\n",
      "        Rounded up this would become 420\n",
      "    2.  Depending on the way we look at the question it can either be 20.955 or 21 epochs. \n",
      "        When we consider the train and test sets to be both run through the trainstep() function.\n",
      "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
      "        then we take the max between those two for a total of 16.76375 (17 when rounded) epochs.\n",
      "    3.  16.76375 or 17 rounded up.\n",
      "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
      "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
      "        we can go up to the humongous number below given basic combinatoric math \n",
      "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
      "        2553595427532697207896236658782847906869511112444955464584374200415869309443961050635633361494919486648172775020586949193836418057452631414395873061047299135306786342851710738555641861925710053699454898916976834843186820794236483017152787331685133409007759209619059384842491896515956129517517163230433337293580498877173735399759757414225084486158140323211053136995791580146060372088374698374755502009016909723850264804283137696375156069094542087730127734747934619245781360199990713961550070797279732888313735307808089033960068946140351729178052128574224151260561895466401459994851285413481732269348863667092355973771930336201103362858691765942903007498770215035417647192799812545990968607367963342646698291656634277284462197604715130710122390060848519718582729317581691333698440433739196687218300909891438014576087197704989419270656984509182985166240686617288733354930896980115700363118288048476431001473029834943976567552146584306952984978744678337273323711187900518071084636671616759626099013124161586231615520221857825905458040683266900653634796593139003481696910181381209934792133549800699813325097453348011915974203791830134162508241371571861313457786276213660433504634786686332541274004003608791390234512906185182479198673837206892256695181088027464244505060519652996131772728880126665665263717526304778979572590569861214649028877419335079290478006763052946286446748781381617188339600271081055403180471724461809622440649158445199072739216936530364086002636104286125758375893649767174729801407936700394156076664338415783722758433370184782025676136800468007128895341604771838325019061855276537180768726575973288754696198654192540791451497686218955230242661795009046648031584150437966475345883096810605264000414511939467950087420138312960911898409447945899642115279857445748286910153943444151744509714113476610776453637793879944601506953734026167299304096848706293275717511175655277953811567292212634750221404541865692829205098157446333804482276548606594031255806228391152552579147493555624767133109274949887199037727073917427285384044155254174434486109737854503427473815914623018386624565997480538487570571804284639462718785769543304146737806671745571086640649496021606936661147899860117808465264351881748123359681192664851591394817416472091093758352442506201419455543051236854175914565614993688543625546931339538358879869134125011454955196880854947717508634423022409914158134535012378354539328643167551799538311731444957613416128761096533116649358184682984082262392334544274681596591882148850798122259960660196747629356329123131973309290281165338808071412330039995720551485164229573899579176297258172060811621459760701386928190117143774321648430772931004954984841002375678259554874694525205917381076885515353917655224155446233745650390234206415145998464522857174592928885427435581829278780824799778437724972195205361350302708666448977830897280840083529213362790397174110042172977446706799195498559864776309446101139848590957399534935291397332282849639336964395744356713820166574680291085640954165938556374407377343902405191387580424737866590661743412347873070797724533856153488312814796800000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
      "        we can see only marginally smaal improvements after epoch 25.\n",
      "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
      "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
      "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\n",
    "    f'''\n",
    "    1.  {len(traindataset)} datapoints divided by batches of size 32 equals {len(traindataset) / 32} batches before the dataset could be fully seen.\n",
    "        Rounded up this would become 420\n",
    "    2.  Depending on the way we look at the question it can either be {(len(traindataset)  + len(testdataset)) / (25 * 32)} or 21 epochs. \n",
    "        When we consider the train and test sets to be both run through the trainstep() function.\n",
    "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
    "        then we take the max between those two for a total of {max(len(testdataset) / (25*32),\n",
    "     len(traindataset) / (25*32))} (17 when rounded) epochs.\n",
    "    3.  {len(traindataset) / (25*32)} or 17 rounded up.\n",
    "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
    "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
    "        we can go up to the humongous number below given basic combinatoric math \n",
    "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
    "        {math.factorial(len(traindataset)) // math.factorial((len(traindataset) - (25*32)))}\n",
    "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
    "        we can see only marginally smaal improvements after epoch 25.\n",
    "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
    "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
    "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir \"/home/mladmin/code/eind_opdracht/examen-22/tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 15.0, 'Predicted'), Text(33.0, 0.5, 'Target')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The confusionmatrix shows the accuracy (as per the documentation of scikitlearn). This confusion matrix shows the a multiclass classification instead of the standard binary variant. Within each of the rows in this matrix the total should be 1 (or 100%). \n",
    "2. I would prefere the F1 metric or even better the F1 weighted metric to have a better feeling how the model performs overal. But, because we know right now that the dataset is imbalanced the accuracy is not a bad score to check our performance and get an idea of the model and if it is performing better than a dumb model.\n",
    "3. because of the data imbalance we know that acc should be higher than ~30 percent for the majority classes and higher than ~5 percent for the minority class. This in turn tells us that the current model is doing a better job than pure gambling\n",
    "4. We could try under or over sampling (I would say under sampling because I do not have the knoweledge regarding over sampling in NLP). Other ways to combat this is maybe using the weighted F1 metric (giving the minority class as much importance as the majority classes) or a weighted loss function. But this wholy depends on the goal of the project. If errors in the minority class are of no problem then tackling this problem give rise to different solutions than when the minority class is the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 14:15:04.962403: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.layers import combinators as cb\n",
    "from trax.shapes import signature\n",
    "from trax.supervised import training\n",
    "from trax.supervised.lr_schedules import warmup_and_rsqrt_decay\n",
    "from trax.layers.assert_shape import assert_shape\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 50), (32,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=50, vocab=v, clean=tokenizer.clean)\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset,\n",
    "    batchsize=32,\n",
    "    preprocessor=preprocessor\n",
    ").stream()\n",
    "\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset,\n",
    "    batchsize=32,\n",
    "    preprocessor=preprocessor\n",
    ").stream()\n",
    "\n",
    "def Cast():\n",
    "    '''\n",
    "    NOTE MAEL:\n",
    "    This function casts an input tensor to an numpy array.\n",
    "    During this process the numpy array will be padded to a fixed lenght\n",
    "    Because the stylepreprocessor uses the torch function pad_sequence()\n",
    "    max length within the current batch, I had to work around this.\n",
    "    changing the tokenizer could be an option but seeing other notebooks might use this\n",
    "    I opted to change this function instead. \n",
    "\n",
    "    The cause to implement this was a trax error where every batch yielded different sized tensors as input X\n",
    "    '''\n",
    "    def f(generator, max_len=50):\n",
    "        for x, y in generator:\n",
    "            new_x= []\n",
    "            _x = x.numpy()\n",
    "            for i, emb in enumerate(_x):\n",
    "                np_array_to_pad = emb\n",
    "                new_x.append(np.pad(np_array_to_pad, (0, max_len - len(np_array_to_pad))))\n",
    "            yield np.array(new_x), y.numpy()\n",
    "        return new_x\n",
    "    return lambda g: f(g)\n",
    "\n",
    "data_pipeline = trax.data.Serial(Cast())\n",
    "trainpipe = data_pipeline(trainstreamer)\n",
    "testpipe = data_pipeline(teststreamer)\n",
    "X, y= next(trainpipe)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 model 1 basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(4)\n",
    ")\n",
    "\n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n",
    "\n",
    "# base_model._clear_init_cache()\n",
    "\n",
    "base_model.init_weights_and_state(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 model 2 with larger first dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model2 = cb.Serial(\n",
    "    tl.Dense(256),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(4)\n",
    ")\n",
    "\n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n",
    "\n",
    "base_model.init_weights_and_state(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 model 3 a RNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-Z0MsbPIA-py3.9/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:1939: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    }
   ],
   "source": [
    "def AvgLast():\n",
    "    return tl.Fn(\"AvgLast\", lambda x: x.mean(axis=-1), n_out=1)\n",
    "\n",
    "@assert_shape(\"bs->bl\")\n",
    "def rnn(vocab_size, feature_size, n_out):\n",
    "    rnn_model = cb.Serial(\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=feature_size),\n",
    "        tl.GRU(n_units=feature_size),\n",
    "        tl.BatchNorm(),\n",
    "        AvgLast(),\n",
    "        tl.Dense(n_out)\n",
    "    )\n",
    "\n",
    "    return rnn_model\n",
    "\n",
    "rnn_model = rnn(len(v), 128, 4)   \n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n",
    "\n",
    "rnn_model.init_weights_and_state(signature(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = cb.Serial(\n",
    "    tl.Embedding(vocab_size= len(v), d_feature= 128),\n",
    "    cb.Branch(\n",
    "        tl.SelfAttention(4),\n",
    "        None\n",
    "    ),\n",
    "    tl.Add(),\n",
    "    tl.LayerNorm(),\n",
    "    cb.Branch(\n",
    "        tl.Dense(128),\n",
    "        None\n",
    "    ),\n",
    "    tl.Relu(),\n",
    "    tl.Add(),\n",
    "    tl.LayerNorm(),\n",
    "    tl.Dense(4),\n",
    "    AvgLast(),\n",
    "    \n",
    "    # tl.SplitIntoHeads(8),\n",
    "    # tl.Attention(128, 8)\n",
    "    # tl.DotProductAttention(),   \n",
    ")\n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.init_weights_and_state(signature(X))\n",
    "attention_model(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = attention_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"../tune_trax/attention_model\")\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=trainpipe,\n",
    "    loss_layer=tl.CategoryCrossEntropy(),\n",
    "    optimizer=trax.optimizers.Adam(),\n",
    "    lr_schedule=lr\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=testpipe,\n",
    "    metrics=[tl.CategoryAccuracy(), tl.WeightedFScore(), tl.MacroAveragedFScore(), tl.CategoryCrossEntropy()],\n",
    "    n_eval_batches=10\n",
    ")\n",
    "\n",
    "loop = training.Loop(\n",
    "    attention_model,\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=OUTPUT_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2620036\n",
      "Step      1: Ran 1 train steps in 6.53 secs\n",
      "Step      1: train CategoryCrossEntropy |  3.90695858\n",
      "Step      1: eval      CategoryAccuracy |  0.07187500\n",
      "Step      1: eval        WeightedFScore |  0.10536353\n",
      "Step      1: eval   MacroAveragedFScore |  0.00728775\n",
      "Step      1: eval  CategoryCrossEntropy |  3.82001815\n",
      "\n",
      "Step    100: Ran 99 train steps in 5.56 secs\n",
      "Step    100: train CategoryCrossEntropy |  2.45681620\n",
      "Step    100: eval      CategoryAccuracy |  0.26250000\n",
      "Step    100: eval        WeightedFScore |  0.31884521\n",
      "Step    100: eval   MacroAveragedFScore |  0.02180175\n",
      "Step    100: eval  CategoryCrossEntropy |  2.39533935\n",
      "\n",
      "Step    200: Ran 100 train steps in 5.74 secs\n",
      "Step    200: train CategoryCrossEntropy |  2.21110559\n",
      "Step    200: eval      CategoryAccuracy |  0.30937500\n",
      "Step    200: eval        WeightedFScore |  0.39211036\n",
      "Step    200: eval   MacroAveragedFScore |  0.02720054\n",
      "Step    200: eval  CategoryCrossEntropy |  2.24503825\n",
      "\n",
      "Step    300: Ran 100 train steps in 5.88 secs\n",
      "Step    300: train CategoryCrossEntropy |  2.12229824\n",
      "Step    300: eval      CategoryAccuracy |  0.31250000\n",
      "Step    300: eval        WeightedFScore |  0.38859533\n",
      "Step    300: eval   MacroAveragedFScore |  0.02724276\n",
      "Step    300: eval  CategoryCrossEntropy |  2.17247622\n",
      "\n",
      "Step    400: Ran 100 train steps in 5.70 secs\n",
      "Step    400: train CategoryCrossEntropy |  2.07578206\n",
      "Step    400: eval      CategoryAccuracy |  0.29687500\n",
      "Step    400: eval        WeightedFScore |  0.37920102\n",
      "Step    400: eval   MacroAveragedFScore |  0.02521799\n",
      "Step    400: eval  CategoryCrossEntropy |  2.26203442\n",
      "\n",
      "Step    500: Ran 100 train steps in 6.21 secs\n",
      "Step    500: train CategoryCrossEntropy |  2.08169222\n",
      "Step    500: eval      CategoryAccuracy |  0.37500000\n",
      "Step    500: eval        WeightedFScore |  0.45582514\n",
      "Step    500: eval   MacroAveragedFScore |  0.03020626\n",
      "Step    500: eval  CategoryCrossEntropy |  2.00118976\n",
      "\n",
      "Step    600: Ran 100 train steps in 5.86 secs\n",
      "Step    600: train CategoryCrossEntropy |  1.94299972\n",
      "Step    600: eval      CategoryAccuracy |  0.31250000\n",
      "Step    600: eval        WeightedFScore |  0.38206989\n",
      "Step    600: eval   MacroAveragedFScore |  0.02474645\n",
      "Step    600: eval  CategoryCrossEntropy |  2.32257872\n",
      "\n",
      "Step    700: Ran 100 train steps in 5.96 secs\n",
      "Step    700: train CategoryCrossEntropy |  1.78004134\n",
      "Step    700: eval      CategoryAccuracy |  0.31250000\n",
      "Step    700: eval        WeightedFScore |  0.37073914\n",
      "Step    700: eval   MacroAveragedFScore |  0.02593241\n",
      "Step    700: eval  CategoryCrossEntropy |  2.27835647\n",
      "\n",
      "Step    800: Ran 100 train steps in 5.80 secs\n",
      "Step    800: train CategoryCrossEntropy |  1.83535171\n",
      "Step    800: eval      CategoryAccuracy |  0.35625000\n",
      "Step    800: eval        WeightedFScore |  0.43023940\n",
      "Step    800: eval   MacroAveragedFScore |  0.02747555\n",
      "Step    800: eval  CategoryCrossEntropy |  2.14418648\n",
      "\n",
      "Step    900: Ran 100 train steps in 5.87 secs\n",
      "Step    900: train CategoryCrossEntropy |  1.82675326\n",
      "Step    900: eval      CategoryAccuracy |  0.32187500\n",
      "Step    900: eval        WeightedFScore |  0.39368642\n",
      "Step    900: eval   MacroAveragedFScore |  0.02535559\n",
      "Step    900: eval  CategoryCrossEntropy |  2.27815616\n",
      "\n",
      "Step   1000: Ran 100 train steps in 5.81 secs\n",
      "Step   1000: train CategoryCrossEntropy |  1.75573170\n",
      "Step   1000: eval      CategoryAccuracy |  0.33125000\n",
      "Step   1000: eval        WeightedFScore |  0.41011126\n",
      "Step   1000: eval   MacroAveragedFScore |  0.02691424\n",
      "Step   1000: eval  CategoryCrossEntropy |  2.42857152\n"
     ]
    }
   ],
   "source": [
    "loop.run(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-28 15:47:40.551516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-28 15:47:42.984674: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-28 15:47:42.984715: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.9.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir \"/home/mladmin/code/eind_opdracht/examen-22/tune_trax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Discussion\n",
    "\n",
    "Due to trax beign the most unfamilair to me I tried implementing 3 trax models. Due to the way the StyleSettings class was set up it was not possible to train multiple models. All config and setting files would be saved to the ../tune folder. This made it impossible to run the train loop on two seperate models. To remedy this I decided to opt for changing the log_dir to a variable that could more closly be controlled. the ../tune/model_1 and ../tune/model_2 folders proofs this is working as intendended.\n",
    "\n",
    "The first model was only for testing a basic architecture using Trax. It only has 3 layers: \n",
    "1. a Dense(128) layer\n",
    "2. a RELU activation layer\n",
    "3. a Dense(4) layer\n",
    "\n",
    "The second model only enlarged the first Dense layer from 128 to 256. This allowed me to test the checkpoint saving and loading used by Trax's loop.run(). Looking at the eval tensorboard files for both models We can at least see that a simple model using a smaller dense layer performs somewhat better than the larger model.\n",
    "\n",
    "The third was done using a RNN with a GRU cell. The goal for me was to try and lay the foundations for an attention network. Getting the RNN to run was the first step. And, looking at the Eval statistics we get a large improvement over the first two models. This should ofcourse come to no suprise as RNN's are better suited for NLP tasks, given their memory of the input. Looking at the logs more training would not produce any better results, as the curve seems to be evening out the model might have learned al it can. We could tinker with the layers, batch sizes or learning rates but, to challenge myself, I will try to implement an attention layer next. \n",
    "\n",
    "Sadly implementing the attention layer did not seem to have a great effect. I tried to implement the right part of the attentionmodel in the standard transformet by Vaswani et al. 2017.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-Z0MsbPIA-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abd276725262becaa5c3e256f8c38384c11f1a26b31876c2a00eb7476ce26550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
