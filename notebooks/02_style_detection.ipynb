{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 15:39:49.992 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humor': 4213, 'reuters': 4186, 'wiki': 4181, 'proverbs': 831}\n",
      "{'humor': 31.414510476474533, 'reuters': 31.21318320781448, 'wiki': 31.175900380284844, 'proverbs': 6.196405935426143}\n"
     ]
    }
   ],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "label_dict = {\n",
    "    \"humor\": 0,\n",
    "    \"reuters\": 0,\n",
    "    \"wiki\": 0,\n",
    "    \"proverbs\": 0\n",
    "}\n",
    "\n",
    "for row in traindataset.dataset:\n",
    "    label_dict[row[1]] += 1\n",
    "\n",
    "print(label_dict)\n",
    "\n",
    "for k in label_dict:\n",
    "    label_dict[k] =  label_dict[k] / len(traindataset.dataset) * 100\n",
    "\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Due to the disbalance in the classes (4th to be precise) the model will probably tend to fit on the first three classes as these are easier to guess. Mostly the minority class will cause problems if we need to predict these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"humor\":\n",
    "            return 0\n",
    "        elif label == \"reuters\":\n",
    "            return 1\n",
    "        elif label == \"wiki\":\n",
    "            return 2\n",
    "        elif label == \"proverbs\":\n",
    "            return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4929,  854,   32,   15,  499,   21, 8496,  890]], dtype=torch.int32),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "preprocessor([(x, y)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 36]),\n",
       " tensor([0, 3, 1, 1, 1, 0, 1, 0, 0, 1, 2, 2, 0, 1, 1, 2, 0, 2, 1, 1, 2, 0, 1, 2,\n",
       "         2, 2, 0, 2, 2, 0, 1, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metric = metrics.F1Score()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class WeightedF1Score(metrics.Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"WeightedF1Score\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        yhat = yhat.argmax(dim=1)\n",
    "        score = f1_score(y, yhat, average=\"weighted\")\n",
    "        return torch.tensor(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.models.metrics.F1Score"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "from src.models import rnn\n",
    "\n",
    "config= {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 4,\n",
    "}\n",
    "\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 20:24:18.185032: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-21 20:24:18.185061: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-06-21 20:24:20.163 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220621-2024\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.86it/s]\n",
      "2022-06-21 20:24:23.090 | INFO     | src.training.train_model:trainloop:164 - Epoch 0 train 1.2988 test 1.2596 metric ['0.2374']\n",
      "100%|██████████| 25/25 [00:02<00:00, 11.47it/s]\n",
      "2022-06-21 20:24:25.978 | INFO     | src.training.train_model:trainloop:164 - Epoch 1 train 1.2341 test 1.2445 metric ['0.1742']\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.21it/s]\n",
      "2022-06-21 20:24:29.080 | INFO     | src.training.train_model:trainloop:164 - Epoch 2 train 1.2354 test 1.1287 metric ['0.3332']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.01it/s]\n",
      "2022-06-21 20:24:31.132 | INFO     | src.training.train_model:trainloop:164 - Epoch 3 train 1.1370 test 1.0322 metric ['0.4838']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.80it/s]\n",
      "2022-06-21 20:24:32.963 | INFO     | src.training.train_model:trainloop:164 - Epoch 4 train 0.9237 test 0.7882 metric ['0.5524']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.49it/s]\n",
      "2022-06-21 20:24:34.862 | INFO     | src.training.train_model:trainloop:164 - Epoch 5 train 0.7118 test 0.6352 metric ['0.6254']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.12it/s]\n",
      "2022-06-21 20:24:36.557 | INFO     | src.training.train_model:trainloop:164 - Epoch 6 train 0.6213 test 0.6043 metric ['0.6275']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.58it/s]\n",
      "2022-06-21 20:24:38.569 | INFO     | src.training.train_model:trainloop:164 - Epoch 7 train 0.6003 test 0.5580 metric ['0.6412']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.68it/s]\n",
      "2022-06-21 20:24:40.470 | INFO     | src.training.train_model:trainloop:164 - Epoch 8 train 0.6028 test 0.5224 metric ['0.6710']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.39it/s]\n",
      "2022-06-21 20:24:42.381 | INFO     | src.training.train_model:trainloop:164 - Epoch 9 train 0.5331 test 0.5153 metric ['0.6847']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.03it/s]\n",
      "2022-06-21 20:24:44.356 | INFO     | src.training.train_model:trainloop:164 - Epoch 10 train 0.5111 test 0.5609 metric ['0.7095']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.41it/s]\n",
      "2022-06-21 20:24:46.284 | INFO     | src.training.train_model:trainloop:164 - Epoch 11 train 0.4985 test 0.5218 metric ['0.6459']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.95it/s]\n",
      "2022-06-21 20:24:48.049 | INFO     | src.training.train_model:trainloop:164 - Epoch 12 train 0.4755 test 0.4382 metric ['0.7778']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.27it/s]\n",
      "2022-06-21 20:24:49.802 | INFO     | src.training.train_model:trainloop:164 - Epoch 13 train 0.4039 test 0.4202 metric ['0.7311']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.90it/s]\n",
      "2022-06-21 20:24:51.567 | INFO     | src.training.train_model:trainloop:164 - Epoch 14 train 0.4113 test 0.4747 metric ['0.7266']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.63it/s]\n",
      "2022-06-21 20:24:53.360 | INFO     | src.training.train_model:trainloop:164 - Epoch 15 train 0.4744 test 0.4653 metric ['0.7660']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.10it/s]\n",
      "2022-06-21 20:24:55.210 | INFO     | src.training.train_model:trainloop:164 - Epoch 16 train 0.3984 test 0.4370 metric ['0.7327']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.34it/s]\n",
      "2022-06-21 20:24:56.963 | INFO     | src.training.train_model:trainloop:164 - Epoch 17 train 0.2715 test 0.5509 metric ['0.7088']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.10it/s]\n",
      "2022-06-21 20:24:58.911 | INFO     | src.training.train_model:trainloop:164 - Epoch 18 train 0.3130 test 0.4905 metric ['0.7207']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.34it/s]\n",
      "2022-06-21 20:25:00.767 | INFO     | src.training.train_model:trainloop:164 - Epoch 19 train 0.3687 test 0.3864 metric ['0.7770']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.52it/s]\n",
      "2022-06-21 20:25:02.541 | INFO     | src.training.train_model:trainloop:164 - Epoch 20 train 0.3323 test 0.3509 metric ['0.7743']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.32it/s]\n",
      "2022-06-21 20:25:04.562 | INFO     | src.training.train_model:trainloop:164 - Epoch 21 train 0.3404 test 0.3846 metric ['0.7792']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.41it/s]\n",
      "2022-06-21 20:25:06.297 | INFO     | src.training.train_model:trainloop:164 - Epoch 22 train 0.3078 test 0.3988 metric ['0.7577']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.86it/s]\n",
      "2022-06-21 20:25:08.328 | INFO     | src.training.train_model:trainloop:164 - Epoch 23 train 0.2975 test 0.3377 metric ['0.8113']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.21it/s]\n",
      "2022-06-21 20:25:10.109 | INFO     | src.training.train_model:trainloop:164 - Epoch 24 train 0.3085 test 0.3260 metric ['0.8168']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.52it/s]\n",
      "2022-06-21 20:25:12.023 | INFO     | src.training.train_model:trainloop:164 - Epoch 25 train 0.3268 test 0.3361 metric ['0.8184']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.54it/s]\n",
      "2022-06-21 20:25:13.984 | INFO     | src.training.train_model:trainloop:164 - Epoch 26 train 0.3061 test 0.3089 metric ['0.8188']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.61it/s]\n",
      "2022-06-21 20:25:15.663 | INFO     | src.training.train_model:trainloop:164 - Epoch 27 train 0.2743 test 0.3243 metric ['0.8161']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.04it/s]\n",
      "2022-06-21 20:25:17.537 | INFO     | src.training.train_model:trainloop:164 - Epoch 28 train 0.2679 test 0.3513 metric ['0.7880']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.79it/s]\n",
      "2022-06-21 20:25:19.289 | INFO     | src.training.train_model:trainloop:164 - Epoch 29 train 0.2635 test 0.3872 metric ['0.7774']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.60it/s]\n",
      "2022-06-21 20:25:21.220 | INFO     | src.training.train_model:trainloop:164 - Epoch 30 train 0.2547 test 0.3147 metric ['0.7964']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.93it/s]\n",
      "2022-06-21 20:25:22.992 | INFO     | src.training.train_model:trainloop:164 - Epoch 31 train 0.2759 test 0.3176 metric ['0.8261']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.89it/s]\n",
      "2022-06-21 20:25:24.804 | INFO     | src.training.train_model:trainloop:164 - Epoch 32 train 0.2784 test 0.2900 metric ['0.8180']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.80it/s]\n",
      "2022-06-21 20:25:26.616 | INFO     | src.training.train_model:trainloop:164 - Epoch 33 train 0.2050 test 0.2312 metric ['0.8495']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.07it/s]\n",
      "2022-06-21 20:25:28.461 | INFO     | src.training.train_model:trainloop:164 - Epoch 34 train 0.1343 test 0.3303 metric ['0.8084']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.78it/s]\n",
      "2022-06-21 20:25:30.276 | INFO     | src.training.train_model:trainloop:164 - Epoch 35 train 0.1831 test 0.3448 metric ['0.8205']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.05it/s]\n",
      "2022-06-21 20:25:32.148 | INFO     | src.training.train_model:trainloop:164 - Epoch 36 train 0.1744 test 0.3466 metric ['0.8260']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.41it/s]\n",
      "2022-06-21 20:25:33.892 | INFO     | src.training.train_model:trainloop:164 - Epoch 37 train 0.1789 test 0.3384 metric ['0.8077']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.94it/s]\n",
      "2022-06-21 20:25:35.701 | INFO     | src.training.train_model:trainloop:164 - Epoch 38 train 0.1553 test 0.3541 metric ['0.8131']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.68it/s]\n",
      "2022-06-21 20:25:37.689 | INFO     | src.training.train_model:trainloop:164 - Epoch 39 train 0.1651 test 0.3286 metric ['0.8239']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.03it/s]\n",
      "2022-06-21 20:25:39.572 | INFO     | src.training.train_model:trainloop:164 - Epoch 40 train 0.1636 test 0.3158 metric ['0.8236']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.81it/s]\n",
      "2022-06-21 20:25:41.210 | INFO     | src.training.train_model:trainloop:164 - Epoch 41 train 0.1784 test 0.3753 metric ['0.7982']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.05it/s]\n",
      "2022-06-21 20:25:43.041 | INFO     | src.training.train_model:trainloop:164 - Epoch 42 train 0.2021 test 0.2972 metric ['0.8376']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.94it/s]\n",
      "2022-06-21 20:25:44.892 | INFO     | src.training.train_model:trainloop:164 - Epoch 43 train 0.1566 test 0.3099 metric ['0.8315']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.26it/s]\n",
      "2022-06-21 20:25:46.805 | INFO     | src.training.train_model:trainloop:164 - Epoch 44 train 0.1859 test 0.2840 metric ['0.8068']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.26it/s]\n",
      "2022-06-21 20:25:48.749 | INFO     | src.training.train_model:trainloop:164 - Epoch 45 train 0.2349 test 0.4414 metric ['0.8089']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.10it/s]\n",
      "2022-06-21 20:25:50.780 | INFO     | src.training.train_model:trainloop:164 - Epoch 46 train 0.1314 test 0.2994 metric ['0.8303']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.49it/s]\n",
      "2022-06-21 20:25:52.961 | INFO     | src.training.train_model:trainloop:164 - Epoch 47 train 0.1632 test 0.2584 metric ['0.8413']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.15it/s]\n",
      "2022-06-21 20:25:54.843 | INFO     | src.training.train_model:trainloop:164 - Epoch 48 train 0.1635 test 0.3307 metric ['0.8282']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.72it/s]\n",
      "2022-06-21 20:25:56.723 | INFO     | src.training.train_model:trainloop:164 - Epoch 49 train 0.1535 test 0.3090 metric ['0.8369']\n",
      "100%|██████████| 50/50 [01:36<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=50,\n",
    "    model=model,\n",
    "    metrics=[metric],\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1.  13411 datapoints divided by batches of size 32 equals 419.09375 batches before the dataset could be fully seen.\n",
      "        Rounded up this would become 420\n",
      "    2.  Depending on the way we look at the question it can either be 20.955 or 21 epochs. \n",
      "        When we consider the train and test sets to be both run through the trainstep() function.\n",
      "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
      "        then we take the max between those two for a total of 16.76375 (17 when rounded) epochs.\n",
      "    3.  16.76375 or 17 rounded up.\n",
      "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
      "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
      "        we can go up to the humongous number below given basic combinatoric math \n",
      "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
      "        2553595427532697207896236658782847906869511112444955464584374200415869309443961050635633361494919486648172775020586949193836418057452631414395873061047299135306786342851710738555641861925710053699454898916976834843186820794236483017152787331685133409007759209619059384842491896515956129517517163230433337293580498877173735399759757414225084486158140323211053136995791580146060372088374698374755502009016909723850264804283137696375156069094542087730127734747934619245781360199990713961550070797279732888313735307808089033960068946140351729178052128574224151260561895466401459994851285413481732269348863667092355973771930336201103362858691765942903007498770215035417647192799812545990968607367963342646698291656634277284462197604715130710122390060848519718582729317581691333698440433739196687218300909891438014576087197704989419270656984509182985166240686617288733354930896980115700363118288048476431001473029834943976567552146584306952984978744678337273323711187900518071084636671616759626099013124161586231615520221857825905458040683266900653634796593139003481696910181381209934792133549800699813325097453348011915974203791830134162508241371571861313457786276213660433504634786686332541274004003608791390234512906185182479198673837206892256695181088027464244505060519652996131772728880126665665263717526304778979572590569861214649028877419335079290478006763052946286446748781381617188339600271081055403180471724461809622440649158445199072739216936530364086002636104286125758375893649767174729801407936700394156076664338415783722758433370184782025676136800468007128895341604771838325019061855276537180768726575973288754696198654192540791451497686218955230242661795009046648031584150437966475345883096810605264000414511939467950087420138312960911898409447945899642115279857445748286910153943444151744509714113476610776453637793879944601506953734026167299304096848706293275717511175655277953811567292212634750221404541865692829205098157446333804482276548606594031255806228391152552579147493555624767133109274949887199037727073917427285384044155254174434486109737854503427473815914623018386624565997480538487570571804284639462718785769543304146737806671745571086640649496021606936661147899860117808465264351881748123359681192664851591394817416472091093758352442506201419455543051236854175914565614993688543625546931339538358879869134125011454955196880854947717508634423022409914158134535012378354539328643167551799538311731444957613416128761096533116649358184682984082262392334544274681596591882148850798122259960660196747629356329123131973309290281165338808071412330039995720551485164229573899579176297258172060811621459760701386928190117143774321648430772931004954984841002375678259554874694525205917381076885515353917655224155446233745650390234206415145998464522857174592928885427435581829278780824799778437724972195205361350302708666448977830897280840083529213362790397174110042172977446706799195498559864776309446101139848590957399534935291397332282849639336964395744356713820166574680291085640954165938556374407377343902405191387580424737866590661743412347873070797724533856153488312814796800000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
      "        we can see only marginally smaal improvements after epoch 25.\n",
      "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
      "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
      "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\n",
    "    f'''\n",
    "    1.  {len(traindataset)} datapoints divided by batches of size 32 equals {len(traindataset) / 32} batches before the dataset could be fully seen.\n",
    "        Rounded up this would become 420\n",
    "    2.  Depending on the way we look at the question it can either be {(len(traindataset)  + len(testdataset)) / (25 * 32)} or 21 epochs. \n",
    "        When we consider the train and test sets to be both run through the trainstep() function.\n",
    "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
    "        then we take the max between those two for a total of {max(len(testdataset) / (25*32),\n",
    "     len(traindataset) / (25*32))} (17 when rounded) epochs.\n",
    "    3.  {len(traindataset) / (25*32)} or 17 rounded up.\n",
    "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
    "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
    "        we can go up to the humongous number below given basic combinatoric math \n",
    "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
    "        {math.factorial(len(traindataset)) // math.factorial((len(traindataset) - (25*32)))}\n",
    "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
    "        we can see only marginally smaal improvements after epoch 25.\n",
    "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
    "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
    "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-21 20:25:58.562842: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-21 20:26:00.784029: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-21 20:26:00.784063: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.9.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir \"/home/mladmin/code/eind_opdracht/examen-22/tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000032vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000032vscode-remote?line=7'>8</a>\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(teststreamer)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000032vscode-remote?line=8'>9</a>\u001b[0m     yhat \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000032vscode-remote?line=9'>10</a>\u001b[0m     yhat \u001b[39m=\u001b[39m yhat\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000032vscode-remote?line=10'>11</a>\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(yhat\u001b[39m.\u001b[39mtolist())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The confusionmatrix shows the accuracy (as per the documentation of scikitlearn). This confusion matrix shows the a multiclass classification instead of the standard binary variant. Within each of the rows in this matrix the total should be 1 (or 100%). \n",
    "2. I would prefere the F1 metric or even better the F1 weighted metric to have a better feeling how the model performs overal. But, because we know right now that the dataset is imbalanced the accuracy is not a bad score to check our performance and get an idea of the model and if it is performing better than a dumb model.\n",
    "3. because of the data imbalance we know that acc should be higher than ~30 percent for the majority classes and higher than ~5 percent for the minority class. This in turn tells us that the current model is doing a better job than pure gambling\n",
    "4. We could try under or over sampling (I would say under sampling because I do not have the knoweledge regarding over sampling in NLP). Other ways to combat this is maybe using the weighted F1 metric (giving the minority class as much importance as the majority classes) or a weighted loss function. But this wholy depends on the goal of the project. If errors in the minority class are of no problem then tackling this problem give rise to different solutions than when the minority class is the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 15:40:11.047319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.layers import combinators as cb\n",
    "from trax.shapes import signature\n",
    "from trax.supervised import training\n",
    "from trax.supervised.lr_schedules import warmup_and_rsqrt_decay\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=24, vocab=v, clean=tokenizer.clean)\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "\n",
    "\n",
    "def Cast():\n",
    "    def f(generator):\n",
    "        for x, y in generator:\n",
    "            yield x.numpy(), y.numpy()\n",
    "\n",
    "    return lambda g: f(g)\n",
    "\n",
    "data_pipeline = trax.data.Serial(Cast())\n",
    "trainpipe = data_pipeline(trainstreamer)\n",
    "testpipe = data_pipeline(teststreamer)\n",
    "X, y = next(trainpipe)\n",
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 24)\n"
     ]
    }
   ],
   "source": [
    "base_model = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(4)\n",
    ")\n",
    "\n",
    "loss = tl.CategoryCrossEntropy()\n",
    "score = tl.WeightedFScore()\n",
    "lr = warmup_and_rsqrt_decay(100, 0.01)\n",
    "\n",
    "\n",
    "log_dir = settings.log_dir\n",
    "\n",
    "base_model.init_weights_and_state(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = training.TrainTask(\n",
    "    labeled_data=trainpipe,\n",
    "    loss_layer=loss,\n",
    "    optimizer=trax.optimizers.Adam(),\n",
    "    lr_schedule=lr\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=testpipe, metrics=[score, loss], n_eval_batches=25\n",
    ")\n",
    "\n",
    "loop = training.Loop(\n",
    "    base_model,\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=log_dir,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-Z0MsbPIA-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abd276725262becaa5c3e256f8c38384c11f1a26b31876c2a00eb7476ce26550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
