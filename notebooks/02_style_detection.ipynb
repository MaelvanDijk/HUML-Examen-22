{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "# import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 18:13:06.776 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humor': 4213, 'reuters': 4186, 'wiki': 4181, 'proverbs': 831}\n",
      "{'humor': 31.414510476474533, 'reuters': 31.21318320781448, 'wiki': 31.175900380284844, 'proverbs': 6.196405935426143}\n"
     ]
    }
   ],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "label_dict = {\n",
    "    \"humor\": 0,\n",
    "    \"reuters\": 0,\n",
    "    \"wiki\": 0,\n",
    "    \"proverbs\": 0\n",
    "}\n",
    "\n",
    "for row in traindataset.dataset:\n",
    "    label_dict[row[1]] += 1\n",
    "\n",
    "print(label_dict)\n",
    "\n",
    "for k in label_dict:\n",
    "    label_dict[k] =  label_dict[k] / len(traindataset.dataset) * 100\n",
    "\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Due to the disbalance in the classes (4th to be precise) the model will probably tend to fit on the first three classes as these are easier to guess. Mostly the minority class will cause problems if we need to predict these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"humor\":\n",
    "            return 0\n",
    "        elif label == \"reuters\":\n",
    "            return 1\n",
    "        elif label == \"wiki\":\n",
    "            return 2\n",
    "        elif label == \"proverbs\":\n",
    "            return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4929,  854,   32,   15,  499,   21, 8496,  890]], dtype=torch.int32),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "preprocessor([(x, y)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 29]),\n",
       " tensor([1, 1, 0, 3, 2, 2, 2, 1, 2, 0, 2, 1, 1, 0, 2, 2, 0, 1, 0, 0, 0, 2, 0, 1,\n",
       "         1, 1, 1, 3, 0, 1, 0, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metric = metrics.F1Score()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class WeightedF1Score(metrics.Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"WeightedF1Score\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        yhat = yhat.argmax(dim=1)\n",
    "        score = f1_score(y, yhat, average=\"weighted\")\n",
    "        return torch.tensor(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.models.metrics.F1Score"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "from src.models import rnn\n",
    "\n",
    "config= {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 4,\n",
    "}\n",
    "\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:31:28.404 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220621-1931\n",
      "100%|██████████| 25/25 [00:03<00:00,  6.59it/s]\n",
      "2022-06-21 19:31:32.826 | INFO     | src.training.train_model:trainloop:164 - Epoch 0 train 1.2320 test 1.1595 metric ['0.2722']\n",
      "100%|██████████| 25/25 [00:01<00:00, 13.00it/s]\n",
      "2022-06-21 19:31:35.144 | INFO     | src.training.train_model:trainloop:164 - Epoch 1 train 1.0748 test 0.9884 metric ['0.3376']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.88it/s]\n",
      "2022-06-21 19:31:37.137 | INFO     | src.training.train_model:trainloop:164 - Epoch 2 train 0.9988 test 0.8866 metric ['0.4141']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.88it/s]\n",
      "2022-06-21 19:31:39.004 | INFO     | src.training.train_model:trainloop:164 - Epoch 3 train 0.8833 test 0.8896 metric ['0.3879']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.03it/s]\n",
      "2022-06-21 19:31:40.798 | INFO     | src.training.train_model:trainloop:164 - Epoch 4 train 0.8719 test 0.8504 metric ['0.5470']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.89it/s]\n",
      "2022-06-21 19:31:42.728 | INFO     | src.training.train_model:trainloop:164 - Epoch 5 train 0.7935 test 0.7640 metric ['0.5313']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.70it/s]\n",
      "2022-06-21 19:31:44.605 | INFO     | src.training.train_model:trainloop:164 - Epoch 6 train 0.7865 test 0.6463 metric ['0.6155']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.63it/s]\n",
      "2022-06-21 19:31:46.256 | INFO     | src.training.train_model:trainloop:164 - Epoch 7 train 0.6019 test 0.5855 metric ['0.6877']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.92it/s]\n",
      "2022-06-21 19:31:48.065 | INFO     | src.training.train_model:trainloop:164 - Epoch 8 train 0.5497 test 0.5433 metric ['0.6543']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.71it/s]\n",
      "2022-06-21 19:31:49.719 | INFO     | src.training.train_model:trainloop:164 - Epoch 9 train 0.5556 test 0.5499 metric ['0.6565']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.40it/s]\n",
      "2022-06-21 19:31:51.492 | INFO     | src.training.train_model:trainloop:164 - Epoch 10 train 0.5330 test 0.4899 metric ['0.6798']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.77it/s]\n",
      "2022-06-21 19:31:53.330 | INFO     | src.training.train_model:trainloop:164 - Epoch 11 train 0.5246 test 0.4902 metric ['0.7109']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.04it/s]\n",
      "2022-06-21 19:31:55.380 | INFO     | src.training.train_model:trainloop:164 - Epoch 12 train 0.4041 test 0.4292 metric ['0.7369']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.67it/s]\n",
      "2022-06-21 19:31:57.132 | INFO     | src.training.train_model:trainloop:164 - Epoch 13 train 0.4325 test 0.5550 metric ['0.6709']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.89it/s]\n",
      "2022-06-21 19:31:58.867 | INFO     | src.training.train_model:trainloop:164 - Epoch 14 train 0.4457 test 0.3747 metric ['0.7342']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.71it/s]\n",
      "2022-06-21 19:32:00.662 | INFO     | src.training.train_model:trainloop:164 - Epoch 15 train 0.3500 test 0.4022 metric ['0.7384']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.44it/s]\n",
      "2022-06-21 19:32:02.397 | INFO     | src.training.train_model:trainloop:164 - Epoch 16 train 0.3430 test 0.4670 metric ['0.7271']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.52it/s]\n",
      "2022-06-21 19:32:04.059 | INFO     | src.training.train_model:trainloop:164 - Epoch 17 train 0.3832 test 0.4734 metric ['0.7362']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.39it/s]\n",
      "2022-06-21 19:32:05.910 | INFO     | src.training.train_model:trainloop:164 - Epoch 18 train 0.3574 test 0.4886 metric ['0.7871']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.23it/s]\n",
      "2022-06-21 19:32:07.589 | INFO     | src.training.train_model:trainloop:164 - Epoch 19 train 0.3644 test 0.3774 metric ['0.7805']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.70it/s]\n",
      "2022-06-21 19:32:09.224 | INFO     | src.training.train_model:trainloop:164 - Epoch 20 train 0.3638 test 0.4216 metric ['0.7508']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.97it/s]\n",
      "2022-06-21 19:32:11.004 | INFO     | src.training.train_model:trainloop:164 - Epoch 21 train 0.3340 test 0.4177 metric ['0.7953']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.69it/s]\n",
      "2022-06-21 19:32:12.817 | INFO     | src.training.train_model:trainloop:164 - Epoch 22 train 0.3263 test 0.4410 metric ['0.7681']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.80it/s]\n",
      "2022-06-21 19:32:14.537 | INFO     | src.training.train_model:trainloop:164 - Epoch 23 train 0.3518 test 0.3567 metric ['0.7754']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.72it/s]\n",
      "2022-06-21 19:32:16.188 | INFO     | src.training.train_model:trainloop:164 - Epoch 24 train 0.3132 test 0.4038 metric ['0.7674']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.45it/s]\n",
      "2022-06-21 19:32:18.141 | INFO     | src.training.train_model:trainloop:164 - Epoch 25 train 0.3002 test 0.3720 metric ['0.7814']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.79it/s]\n",
      "2022-06-21 19:32:19.980 | INFO     | src.training.train_model:trainloop:164 - Epoch 26 train 0.3320 test 0.3681 metric ['0.7963']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.95it/s]\n",
      "2022-06-21 19:32:21.859 | INFO     | src.training.train_model:trainloop:164 - Epoch 27 train 0.2703 test 0.3834 metric ['0.8071']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.84it/s]\n",
      "2022-06-21 19:32:23.581 | INFO     | src.training.train_model:trainloop:164 - Epoch 28 train 0.3090 test 0.3603 metric ['0.7998']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.85it/s]\n",
      "2022-06-21 19:32:25.391 | INFO     | src.training.train_model:trainloop:164 - Epoch 29 train 0.3043 test 0.3814 metric ['0.7909']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.11it/s]\n",
      "2022-06-21 19:32:27.134 | INFO     | src.training.train_model:trainloop:164 - Epoch 30 train 0.2112 test 0.3958 metric ['0.7762']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.89it/s]\n",
      "2022-06-21 19:32:28.922 | INFO     | src.training.train_model:trainloop:164 - Epoch 31 train 0.1961 test 0.3871 metric ['0.8047']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.21it/s]\n",
      "2022-06-21 19:32:30.615 | INFO     | src.training.train_model:trainloop:164 - Epoch 32 train 0.2473 test 0.3820 metric ['0.8028']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.08it/s]\n",
      "2022-06-21 19:32:32.423 | INFO     | src.training.train_model:trainloop:164 - Epoch 33 train 0.2064 test 0.3769 metric ['0.7419']\n",
      "100%|██████████| 25/25 [00:01<00:00, 20.18it/s]\n",
      "2022-06-21 19:32:34.069 | INFO     | src.training.train_model:trainloop:164 - Epoch 34 train 0.1807 test 0.3730 metric ['0.8314']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.69it/s]\n",
      "2022-06-21 19:32:35.723 | INFO     | src.training.train_model:trainloop:164 - Epoch 35 train 0.1472 test 0.4421 metric ['0.7913']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.04it/s]\n",
      "2022-06-21 19:32:37.445 | INFO     | src.training.train_model:trainloop:164 - Epoch 36 train 0.1705 test 0.3424 metric ['0.7771']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.87it/s]\n",
      "2022-06-21 19:32:39.065 | INFO     | src.training.train_model:trainloop:164 - Epoch 37 train 0.1588 test 0.4408 metric ['0.7635']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.09it/s]\n",
      "2022-06-21 19:32:40.917 | INFO     | src.training.train_model:trainloop:164 - Epoch 38 train 0.2158 test 0.4098 metric ['0.7871']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.02it/s]\n",
      "2022-06-21 19:32:42.769 | INFO     | src.training.train_model:trainloop:164 - Epoch 39 train 0.2333 test 0.3735 metric ['0.8258']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.50it/s]\n",
      "2022-06-21 19:32:44.425 | INFO     | src.training.train_model:trainloop:164 - Epoch 40 train 0.2043 test 0.3428 metric ['0.8313']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.49it/s]\n",
      "2022-06-21 19:32:46.242 | INFO     | src.training.train_model:trainloop:164 - Epoch 41 train 0.2026 test 0.3372 metric ['0.8134']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.45it/s]\n",
      "2022-06-21 19:32:48.024 | INFO     | src.training.train_model:trainloop:164 - Epoch 42 train 0.2032 test 0.3397 metric ['0.8155']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.04it/s]\n",
      "2022-06-21 19:32:49.744 | INFO     | src.training.train_model:trainloop:164 - Epoch 43 train 0.1593 test 0.3461 metric ['0.8089']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.75it/s]\n",
      "2022-06-21 19:32:51.466 | INFO     | src.training.train_model:trainloop:164 - Epoch 44 train 0.2086 test 0.3807 metric ['0.7986']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.37it/s]\n",
      "2022-06-21 19:32:53.268 | INFO     | src.training.train_model:trainloop:164 - Epoch 45 train 0.1586 test 0.3172 metric ['0.8229']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.95it/s]\n",
      "2022-06-21 19:32:54.977 | INFO     | src.training.train_model:trainloop:164 - Epoch 46 train 0.1693 test 0.4428 metric ['0.7785']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.49it/s]\n",
      "2022-06-21 19:32:56.830 | INFO     | src.training.train_model:trainloop:164 - Epoch 47 train 0.1300 test 0.3267 metric ['0.8519']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.50it/s]\n",
      "2022-06-21 19:32:58.582 | INFO     | src.training.train_model:trainloop:164 - Epoch 48 train 0.0971 test 0.4420 metric ['0.8316']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.71it/s]\n",
      "2022-06-21 19:33:00.379 | INFO     | src.training.train_model:trainloop:164 - Epoch 49 train 0.1116 test 0.4161 metric ['0.8040']\n",
      "100%|██████████| 50/50 [01:31<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=50,\n",
    "    model=model,\n",
    "    metrics=[metric],\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1.  13411 datapoints divided by batches of size 32 equals 419.09375 batches before the dataset could be fully seen.\n",
      "        Rounded up this would become 420\n",
      "    2.  Depending on the way we look at the question it can either be 20.955 or 21 epochs. \n",
      "        When we consider the train and test sets to be both run through the trainstep() function.\n",
      "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
      "        then we take the max between those two for a total of 16.76375 (17 when rounded) epochs.\n",
      "    3.  16.76375 or 17 rounded up.\n",
      "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
      "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
      "        we can go up to the humongous number below given basic combinatoric math \n",
      "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
      "        2553595427532697207896236658782847906869511112444955464584374200415869309443961050635633361494919486648172775020586949193836418057452631414395873061047299135306786342851710738555641861925710053699454898916976834843186820794236483017152787331685133409007759209619059384842491896515956129517517163230433337293580498877173735399759757414225084486158140323211053136995791580146060372088374698374755502009016909723850264804283137696375156069094542087730127734747934619245781360199990713961550070797279732888313735307808089033960068946140351729178052128574224151260561895466401459994851285413481732269348863667092355973771930336201103362858691765942903007498770215035417647192799812545990968607367963342646698291656634277284462197604715130710122390060848519718582729317581691333698440433739196687218300909891438014576087197704989419270656984509182985166240686617288733354930896980115700363118288048476431001473029834943976567552146584306952984978744678337273323711187900518071084636671616759626099013124161586231615520221857825905458040683266900653634796593139003481696910181381209934792133549800699813325097453348011915974203791830134162508241371571861313457786276213660433504634786686332541274004003608791390234512906185182479198673837206892256695181088027464244505060519652996131772728880126665665263717526304778979572590569861214649028877419335079290478006763052946286446748781381617188339600271081055403180471724461809622440649158445199072739216936530364086002636104286125758375893649767174729801407936700394156076664338415783722758433370184782025676136800468007128895341604771838325019061855276537180768726575973288754696198654192540791451497686218955230242661795009046648031584150437966475345883096810605264000414511939467950087420138312960911898409447945899642115279857445748286910153943444151744509714113476610776453637793879944601506953734026167299304096848706293275717511175655277953811567292212634750221404541865692829205098157446333804482276548606594031255806228391152552579147493555624767133109274949887199037727073917427285384044155254174434486109737854503427473815914623018386624565997480538487570571804284639462718785769543304146737806671745571086640649496021606936661147899860117808465264351881748123359681192664851591394817416472091093758352442506201419455543051236854175914565614993688543625546931339538358879869134125011454955196880854947717508634423022409914158134535012378354539328643167551799538311731444957613416128761096533116649358184682984082262392334544274681596591882148850798122259960660196747629356329123131973309290281165338808071412330039995720551485164229573899579176297258172060811621459760701386928190117143774321648430772931004954984841002375678259554874694525205917381076885515353917655224155446233745650390234206415145998464522857174592928885427435581829278780824799778437724972195205361350302708666448977830897280840083529213362790397174110042172977446706799195498559864776309446101139848590957399534935291397332282849639336964395744356713820166574680291085640954165938556374407377343902405191387580424737866590661743412347873070797724533856153488312814796800000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
      "        we can see only marginally smaal improvements after epoch 25.\n",
      "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
      "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
      "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\n",
    "    f'''\n",
    "    1.  {len(traindataset)} datapoints divided by batches of size 32 equals {len(traindataset) / 32} batches before the dataset could be fully seen.\n",
    "        Rounded up this would become 420\n",
    "    2.  Depending on the way we look at the question it can either be {(len(traindataset)  + len(testdataset)) / (25 * 32)} or 21 epochs. \n",
    "        When we consider the train and test sets to be both run through the trainstep() function.\n",
    "        If we consider the trainset in the evalbatches() and the trainset in the trainstep() functions\n",
    "        then we take the max between those two for a total of {max(len(testdataset) / (25*32),\n",
    "     len(traindataset) / (25*32))} (17 when rounded) epochs.\n",
    "    3.  {len(traindataset) / (25*32)} or 17 rounded up.\n",
    "    4.  25 epochs should be reasonable maybe 50 to be sure we have seen more combinations within the dataset. \n",
    "        If we want to go big and come close to the total amount of permutations the traindataset contains\n",
    "        we can go up to the humongous number below given basic combinatoric math \n",
    "        (https://statisticsbyjim.com/probability/permutations-probabilities/): \n",
    "        {math.factorial(len(traindataset)) // math.factorial((len(traindataset) - (25*32)))}\n",
    "    5.  in the image \"tune/20220621-1931\"  (could not save the image to the figure folder, sorry!)\n",
    "        we can see only marginally smaal improvements after epoch 25.\n",
    "        Even worse we see signals of overfitting where the test dataset loss is not improving (only wobbling)\n",
    "        the train loss is decreasing comparatively a lot (the two are even diverging starting from epoch 45).\n",
    "        This indicates that the model is learning the patterns in the trainset and not generalizing on the data.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000030vscode-remote?line=15'>16</a>\u001b[0m cfm \u001b[39m=\u001b[39m confusion_matrix(y, yhat)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000030vscode-remote?line=16'>17</a>\u001b[0m cfm_norm \u001b[39m=\u001b[39m cfm \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(cfm, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000030vscode-remote?line=17'>18</a>\u001b[0m plot \u001b[39m=\u001b[39m sns\u001b[39m.\u001b[39mheatmap(cfm_norm, annot\u001b[39m=\u001b[39mcfm_norm, fmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.3f\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d7644696a6b48554d4c227d/home/mladmin/code/eind_opdracht/examen-22/notebooks/02_style_detection.ipynb#ch0000030vscode-remote?line=18'>19</a>\u001b[0m plot\u001b[39m.\u001b[39mset(xlabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicted\u001b[39m\u001b[39m\"\u001b[39m, ylabel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-Z0MsbPIA-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abd276725262becaa5c3e256f8c38384c11f1a26b31876c2a00eb7476ce26550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
